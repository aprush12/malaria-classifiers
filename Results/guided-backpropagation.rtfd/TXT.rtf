{\rtf1\ansi\ansicpg1252\cocoartf2580
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\froman\fcharset0 Times-BoldItalic;\f1\froman\fcharset0 Times-Roman;\f2\froman\fcharset0 TimesNewRomanPSMT;
\f3\fswiss\fcharset0 Helvetica;\f4\froman\fcharset0 Times-Italic;}
{\colortbl;\red255\green255\blue255;\red0\green0\blue0;\red255\green255\blue255;}
{\*\expandedcolortbl;;\cssrgb\c0\c0\c0;\cssrgb\c100000\c100000\c100000;}
\margl1440\margr1440\vieww11520\viewh8400\viewkind0
\deftab720
\pard\pardeftab720\sa240\partightenfactor0

\f0\i\b\fs32 \cf2 \cb3 \expnd0\expndtw0\kerning0
Guided Backpropagation and Saliency Maps: Peeking into the Black Box 
\f1\i0\b0\fs24 \cb1 \
\pard\pardeftab720\sa240\partightenfactor0

\f2\fs32 \cf2 \cb3 A significant criticism of CNNs in healthcare is their 'black box' nature: the inability to trace the logic behind neural network decisions presents significant challenges in highly sensitive applications like medicine, where trust and reliability are crucial. To address this, we implemented saliency maps in our model, which visualize the input data that most significantly impacts the model\'92s decision-making process. Unlike regular backpropagation, where all gradients flow backward through the network after a prediction, guided backpropagation selectively allows only positive gradients to flow back. By focusing only on features that positively impact the network weights, this approach identifies which aspects of the input data contribute most to the model\'92s decision-making.
\f1\fs24 \cb1 \
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardeftab720\pardirnatural\partightenfactor0

\f3 \cf0 \kerning1\expnd0\expndtw0 {{\NeXTGraphic Pasted Graphic 5.png \width9580 \height10820 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardeftab720\pardirnatural\partightenfactor0
\cf0 \
\pard\pardeftab720\sa240\partightenfactor0

\f2\fs32 \cf2 \cb3 \expnd0\expndtw0\kerning0
In the fourth image of the 
\f4\i Infected/Parasitized Images 
\f2\i0 table, we see that the model draws heavily from the non-parasitized portions of the cell to make its decision. Although the image was correctly identified as a parasitic cell, the saliency map reveals weaknesses in the classifier\'92s decision-making process. Hence, the classifier could benefit from more training data with parasites near the cell's periphery or images where the contrast between the parasite and cell background is less distinct. }